---
title: "HOMEWORK 1 - Creating Value Through Data Mining (S402010)"
author: "Liam Phan"
date: "`r Sys.Date()`"
output:
  rmdformats::material :
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
---

# Quick Start

> Most of the plots are interactive, you can click or zoom to get more details ! Also don't hesitate to click on plots, they will zoom automatically ! 

## Loading Packages

```{r 2, warning=FALSE, message=FALSE }

# Loading Packages 

library(data.table)
library(lubridate)
library(tidyverse)
library(esquisse)
library(plyr)
library(ggplot2)
library(cowplot)
library(naniar) #for NA exploration
library(sp) #spatial data
library(reshape2)
library(plotly)
library(gissr)
library(leaflet)
library(leaflet.providers)
library(geosphere)
library(DT)
library(gridExtra)
library(corrplot)
library(RColorBrewer)
library(rmdformats)

```

> Those are required packages


# Ex 3.4

```{r clean, include=FALSE}

rm(list = ls()) # clean environment

```

## Loading Datas and Cleaning

> Loading the dataset called "LaptopSales_red.csv" given for the Homework

```{r 3,echo=FALSE, warning=FALSE, comment=FALSE}

Laptop_Sales_Data <- fread("DATA/LaptopSales_red.csv")

#is.data.table(Laptop_Sales_Data)

#summary(Laptop_Sales_Data)

str(Laptop_Sales_Data)

```

<center>

```{r 333, echo=FALSE, warning=FALSE, comment=FALSE}

gg_miss_var(Laptop_Sales_Data, show_pct = TRUE)

```


> Retail Price is the only variable missing at rate of approximately 4% 

</center>

## a.Price Questions:

```{r 4, include=FALSE}

#### Set Up a Data Subset and NA OMIT

Retail_Price_and_Dates <- Laptop_Sales_Data[,.(Retail.Price,Date)][,Date:=mdy_hm(Date)]

Retail_Price_and_Dates <- na.omit(Retail_Price_and_Dates)

```


### i. At What Price are the laptops actually selling ?

<center>

```{r 5, echo=FALSE}

# Histogram of the Retail Price of Computer In 2018
ggplotly(
ggplot(Retail_Price_and_Dates) +
 aes(x = Retail.Price) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 labs(x = "Price", y = "Frequency", title = "Histogram of the Retail Price of Computer", subtitle = "In 2018") + theme_classic()
)

```

> This barplot shows the most frequent retail prices for all stores in 2018

</center>

<center>

```{r 51, echo=FALSE, warning=FALSE, comment=FALSE}

# Boxplot of the Retail Price of Computer In 2018

ggplotly(
ggplot(Retail_Price_and_Dates) +
 aes(x = "", y = Retail.Price) +
 geom_boxplot(fill = "#1c6155") + stat_summary(fun=mean, geom="point", shape=20, size=8, color="white", fill="white") + 
 labs(y = "Price", x="",
 title = "Boxplot of the Retail Price of Computer", subtitle = "In 2018") +
 theme_classic()
)

```

</center>

> We can interpret this boxplot as the mean or median retail price of the 2018 Computer Dataset, click on the white sphere to get the mean !

```{r 52, include=FALSE}

# Actual price

Max_Date_Retail <- max(Retail_Price_and_Dates$Date)

Actual_Price <- Retail_Price_and_Dates[Date %in% Max_Date_Retail, ]

```

```{r 53,echo=FALSE}

result1 <- print(paste("Last Recorded Prices are", Actual_Price[1,1], "USD", "and", Actual_Price[2,1],"USD","on the same Day with a mean of",mean(Actual_Price$Retail.Price),"USD"))

```

> Here is given the last recorded prices for 2018


### ii. Does price change with time?


```{r 61, include=FALSE}

# Aggregating by Month
Retail_Price_and_Dates_Month <- Retail_Price_and_Dates[, mean(Retail.Price), by = floor_date(Date,unit="month")]

colnames(Retail_Price_and_Dates_Month)[1] <- "Date"
colnames(Retail_Price_and_Dates_Month)[2] <- "Mean_Retail_Price"

# Aggregating by Week
Retail_Price_and_Dates_Week <- Retail_Price_and_Dates[, mean(Retail.Price), by = floor_date(Date,unit = "week")]

colnames(Retail_Price_and_Dates_Week)[1] <- "Date"
colnames(Retail_Price_and_Dates_Week)[2] <- "Mean_Retail_Price"

# Aggregating by Day
Retail_Price_and_Dates_Day <- Retail_Price_and_Dates[, mean(Retail.Price), by = floor_date(Date,unit = "day")]

colnames(Retail_Price_and_Dates_Day)[1] <- "Date"
colnames(Retail_Price_and_Dates_Day)[2] <- "Mean_Retail_Price"

# Aggregating by Weekday

Retail_Price_and_Dates_WeekDay <- Retail_Price_and_Dates[,Weekday:=weekdays(Date)]
Retail_Price_and_Dates_WeekDay <- Retail_Price_and_Dates_WeekDay[,mean(Retail.Price),by=Weekday]

Retail_Price_and_Dates_WeekDay <- Retail_Price_and_Dates_WeekDay %>% mutate(Translation=c("Saturday","Friday","Wednesday","Monday","Tuesday","Thursday","Sunday"))

Retail_Price_and_Dates_WeekDay$Translation <- factor(Retail_Price_and_Dates_WeekDay$Translation, ordered = TRUE, levels=(c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))

colnames(Retail_Price_and_Dates_WeekDay)[2] <- "Mean_Retail_Price"

```

<center>

```{r 6, echo=FALSE}

# Retail Price By Month
RetailPricePlot1 <- ggplotly(
  ggplot(Retail_Price_and_Dates_Month) +
 aes(x = Date, y = Mean_Retail_Price) +
 geom_line(size = 1.1, 
 colour = "#1c6155") + geom_point() +
 labs(x = "Months", y = "Average Retail Pric", title = "Average Retail Price of Computer in 2018", 
 subtitle = "Aggregated by Month") +
 theme_classic()
 )

RetailPricePlot1

# Retail Price By Week
RetailPricePlot2 <- ggplotly(
  ggplot(Retail_Price_and_Dates_Week) +
 aes(x = Date, y = Mean_Retail_Price) +
 geom_line(size = 0.4, 
 colour = "#1c6155") + geom_point() +
 labs(x = "Weeks", y = "Average Retail Pric", title = "Average Retail Price of Computer in 2018", 
 subtitle = "Aggregated by Week") +
 theme_classic()
 )

RetailPricePlot2

# Retail Price By Day
RetailPricePlot3 <- ggplotly(
  ggplot(Retail_Price_and_Dates_Day) +
 aes(x = Date, y = Mean_Retail_Price) +
 geom_line(size = 0.2, 
 colour = "#1c6155")  +
 labs(x = "Days", y = "Average Retail Price", title = "Average Retail Price of Computer in 2018", 
 subtitle = "Aggregated by Day") + 
 theme_classic()
 )

RetailPricePlot3

# Retail Price By Week Day

RetailPricePlot4 <- ggplotly(
  ggplot(Retail_Price_and_Dates_WeekDay) +
 aes(x = Translation, y = Mean_Retail_Price) +
 geom_col(fill = "#1c6155") +
 labs(x = "Weekdays", y = "Average Retail Price", title = "Average Retail Price during Weekdays", subtitle = "In 2018") + 
 theme_classic() + coord_cartesian(ylim = c(505, 510))
)

RetailPricePlot4

```

</center>

> Those Plot shows different aggregations levels, can be used depending on the analysis we want, thus the granularity need.

### iii. Are prices consistent across retail outlets?


```{r 7, include=FALSE}

#### Set Up a Data Subset and NA OMIT

Retail_Price_Outlets_Date <- Laptop_Sales_Data[,.(Retail.Price,Store.Postcode,Date)][,Date:=mdy_hm(Date)]

Retail_Price_Outlets_Date  <- na.omit(Retail_Price_Outlets_Date)

Retail_Price_Configuration <- Laptop_Sales_Data[,.(Retail.Price,Configuration,Screen.Size..Inches.,Battery.Life..Hours.,RAM..GB.,Processor.Speeds..GHz.,Integrated.Wireless.,HD.Size..GB.,Bundled.Applications.)]

Retail_Price_Configuration <- na.omit(Retail_Price_Configuration)

```


<center>

```{r 8,echo=FALSE}

#### Boxplot Across Retail Outlets

ggplotly(
ggplot(Retail_Price_Outlets_Date) +
 aes(x = Store.Postcode, y = Retail.Price) +
 geom_boxplot(fill = "#1c6155") +
 labs(x = "Stores Postcode", y = "Price", title = "Boxplot Of The Retail Price Across Stores", subtitle = "In 2018") +
 theme_classic() + scale_x_discrete() + theme(axis.text.x=element_text(size=rel(1), angle=90))
)

```

</center>

> Each box plots belongs to a specific stores, we can see a common trend across all stores in 2018


```{r 81, include=FALSE}

#### Retail Price Across Stores During 2018 - Data

Retail_Price_Outlets_Date_Month <- Retail_Price_Outlets_Date[,Floor.Date:=floor_date(Date,unit="month")][,c(Mean_Price=mean(Retail.Price)), by=list(Store.Postcode,Floor.Date)]

colnames(Retail_Price_Outlets_Date_Month)[3] <- "Mean_Retail_Price"

```

<center>


```{r 83, echo=FALSE}

#### Plot of the Monthly Retail Price per Stores
ggplotly(
ggplot(Retail_Price_Outlets_Date_Month) +
 aes(x = Floor.Date, y = Mean_Retail_Price, colour = Store.Postcode) +
 geom_line(size = 0.5) +
 scale_color_hue(direction = 1) +
 labs(x = "Month", y = "Price", title = "Retail Price Across Months and Grouped by Stores", 
 subtitle = "In 2018") +
 theme_classic() 
)

```

</center>

> Looking at times series, we can see that not all stores have the same time trend, but most of them do.


### iv. How does price change with configuration?


<center>

```{r 82, echo=FALSE, warning=FALSE, comment=FALSE} 

#### Plot Of The Retail Price per Configuration

ggplot(Retail_Price_Configuration) +
 aes(x = Configuration, y = Retail.Price) +
 geom_point(shape = "circle", size = 0.6) +
 scale_color_gradient() +
 labs(y = "Retail Price", title = "Retail Price and Configuration ", 
 subtitle = "In 2018") + geom_smooth(color='#1c6155') + theme_classic()

``` 

</center>

> Using an smooth approximator, we can see two differents trends, first a rapid increase in price while being at low configurations, and then the slope tend to stay constant and low, ending with a increase with highest configurations. 


## b.Location Questions

### i. Where are the stores and customers locatd?


```{r 9, echo=FALSE} 

#Transform UK to Worldwide Geodata

## For Client Data

Data_Client_Coordinates <- Laptop_Sales_Data[,.(customer.X,customer.Y)]
Data_Client_Coordinates <- na.omit(Data_Client_Coordinates)
Data_Client_Coordinates <- distinct(Data_Client_Coordinates)
Data_Client_Coordinates <- transform_coordinates(Data_Client_Coordinates,
                                                        latitude="customer.Y",longitude = "customer.X", from = projection_bng(), to = projection_wgs84())
Data_Client_Coordinates$customer.X <- as.numeric(Data_Client_Coordinates$customer.X)
Data_Client_Coordinates$customer.Y <- as.numeric(Data_Client_Coordinates$customer.Y)

## For Store Data

Data_Stores_Coordinates <- Laptop_Sales_Data[,.(store.X,store.Y)]
Data_Stores_Coordinates <- na.omit(Data_Stores_Coordinates)
Data_Stores_Coordinates <- distinct(Data_Stores_Coordinates)
Data_Stores_Coordinates <- transform_coordinates(Data_Stores_Coordinates,
                                                        latitude="store.Y",longitude = "store.X", from = projection_bng(), to = projection_wgs84())

Data_Stores_Coordinates$store.X <- as.numeric(Data_Stores_Coordinates$store.X)
Data_Stores_Coordinates$store.Y <- as.numeric(Data_Stores_Coordinates$store.Y)


```

<center>

```{r 84,echo=FALSE} 

#Ploting Map 

map_1 <- leaflet() %>% addProviderTiles(providers$CartoDB.Positron) %>% addMarkers(data=Data_Client_Coordinates, lng = ~Data_Client_Coordinates$customer.X, lat = ~Data_Client_Coordinates$customer.Y,icon = list(iconUrl='https://cdn-icons-png.flaticon.com/512/4573/4573516.png',iconSize=c(25,25)), clusterOptions = markerClusterOptions(),popup = ~paste("<h3>Client Coordinates</h3>","<b>Latitude:</b>",customer.X,"<b>Longitude:</b>",customer.Y)) %>% addMarkers(data=Data_Stores_Coordinates, lng = ~Data_Stores_Coordinates$store.X, lat = ~Data_Stores_Coordinates$store.Y,icon = list(iconUrl='https://cdn-icons-png.flaticon.com/512/726/726569.png',iconSize=c(25,25)), clusterOptions = markerClusterOptions(), popup = ~paste("<h3>Store Coordinates</h3>","<b>Latitude:</b>",store.X,"<b>Longitude:</b>",store.Y))

map_1

```

</center>

> Enjoy looking at each stores and customers in London UK ! You can find there exact location by clicking on them ! 

### ii. Which stores are selling the most?


```{r 10, include=FALSE}

Laptop_Sales_Data_2 <- Laptop_Sales_Data

Sales_Stores <- Laptop_Sales_Data_2[, .N, by=Store.Postcode]

Sales_Stores_2 <- Laptop_Sales_Data_2[,.(Revenues=sum(Retail.Price, na.rm = TRUE)),by=Store.Postcode]

```


<center>

```{r 103, echo=FALSE}

# Plot Transactions per Stores

ggplotly(
ggplot(Sales_Stores) +
 aes(x = reorder(Store.Postcode,-N), y = N) +
 geom_col(fill = "#1c6155") +
 labs(x = "Stores", 
 y = "Number Of Transactions", title = "Number of Transactions per Store", subtitle = "In 2018") +
 theme_classic() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
)

# Plot Revenues per Stores

ggplotly(
ggplot(Sales_Stores_2) +
 aes(x = reorder(Store.Postcode,-Revenues), y = Revenues) +
 geom_col(fill = "#1c6155") +
 labs(x = "Store Postcode", 
 y = "Revenues", title = "Revenues per Stores", subtitle = "In 2018") + 
 theme_classic()+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
)

```

</center>

> The following barplots show two ways of analyzing the stores sales results: by the number of transactions or the sales revenues they each generated during 2018.


### iii. How far would customers travel to buy a laptop ?


```{r 11}





```


### iv. How far would customers travel to buy a laptop ? - Alternative


```{r 111, include=FALSE}

# Distance Calculations

Distance_Customer_Shop_Data <- Laptop_Sales_Data[, .(customer.X,customer.Y,store.X,store.Y)]

Distance_Customer_Shop_Data <- distinct(Distance_Customer_Shop_Data)

Distance_Customer_Shop_Data <- na.omit(Distance_Customer_Shop_Data)

Distance_Customer_Shop_Data$customer.X <- as.numeric(Distance_Customer_Shop_Data$customer.X)
Distance_Customer_Shop_Data$customer.Y <- as.numeric(Distance_Customer_Shop_Data$customer.Y)
Distance_Customer_Shop_Data$store.X <- as.numeric(Distance_Customer_Shop_Data$store.X)
Distance_Customer_Shop_Data$store.Y <- as.numeric(Distance_Customer_Shop_Data$store.Y)

Distance_Customer <- transform_coordinates(Distance_Customer_Shop_Data, latitude="customer.Y",longitude = "customer.X", from = projection_bng(), to = projection_wgs84())

Distance_Customer <- Distance_Customer[,c(1,2)]

Distance_Store <- transform_coordinates(Distance_Customer_Shop_Data, latitude="store.Y",longitude = "store.X", from = projection_bng(), to = projection_wgs84())

Distance_Store <- Distance_Store[,c(3,4)]

All_Distance <- cbind(Distance_Customer,Distance_Store)

All_Distance <- All_Distance %>% rowwise() %>% mutate(Distance=distHaversine(c(customer.Y,customer.X),c(store.Y,store.X)))

```

```{r 1111,echo=FALSE}

datatable(All_Distance, colnames = c('Customer X ', 'Customer Y', 'Store X', 'Store Y', 'Distance Between Them in Meters'))

```


> Each Unique Customer can be found here, scroll down and see the distance they need to travel to get to their store.

<center>

```{r 1121,echo=FALSE}

ggplotly(
  
ggplot(All_Distance) +
 aes(x = Distance) +
 geom_histogram(bins = 60L, fill = "#1c6155") +
 labs(x = "Distance (in meters)", 
 title = "Histogram of the Distance between Clients and Stores", subtitle = "In 2018") +
 theme_classic()

)


```

</center>

> Histogram of the Distance between Clients and Stores

## c.Revenue Questions


### i. How do the sales volume in each store relate to Acell's revenues?


```{r 12, include=FALSE}

Sales_Stores_Revenues <- Laptop_Sales_Data[,.(Revenues=sum(Retail.Price, na.rm = TRUE)), by= Store.Postcode]

Total_Revenue <- sum(Sales_Stores_Revenues$Revenues)

Sales_Stores_Revenues <- Sales_Stores_Revenues[,Percentage_Revenue:=Revenues/Total_Revenue]

Sales_Stores_Revenues$Percentage_Revenue <- Sales_Stores_Revenues$Percentage_Revenue*100

```


<center>

```{r 121,echo=FALSE}

ggplotly(
  ggplot(Sales_Stores_Revenues) +
 aes(x = reorder(Store.Postcode,-Percentage_Revenue), y = Percentage_Revenue) +
 geom_col(fill = "#1c6155") +
 labs(x = "Store Postcode", y = "% of Total Revenues", title = "Revenues Contribution per Stores", 
 subtitle = "In 2018") + 
 theme_classic()+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
 )

```

</center>

> You can see the proportional revenues participation of each stores in 2018. 


### ii. How does this relationship depend on the configuration?


<center>

```{r 1313, echo=FALSE}



```

</center>

## d.Configuration Questions 


### i. What are the details of each configuration? How does this relate to price?


<center>

```{r 14,echo=FALSE}

Detail_Price <- Laptop_Sales_Data[,.(Retail.Price,Screen.Size..Inches.,Battery.Life..Hours.,RAM..GB.,Processor.Speeds..GHz.,HD.Size..GB.,Integrated.Wireless.,Bundled.Applications.)]

Detail_Price <- na.omit(Detail_Price)

# Screen Size

detail_plot1 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Screen Size (Inches)", y = "Retail Price", title = "Retail Price compared to Screen Size", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Screen.Size..Inches.)) + theme(text = element_text(size = 6))    
 
# Battery Life

detail_plot2 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Battery Life (Hours)", y = "Retail Price", title = "Retail Price compared to Battery Life", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Battery.Life..Hours.)) + theme(text = element_text(size = 6))    

# RAM 

detail_plot3 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "RAM (GB)", y = "Retail Price", title = "Retail Price compared to RAM", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(RAM..GB.)) + theme(text = element_text(size = 6))   

# Processor Speed

detail_plot4 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Processor Speeds (GHz)", y = "Retail Price", title = "Retail Price compared to Processor Speed", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Processor.Speeds..GHz.)) + theme(text = element_text(size = 6))    

# HD Size

detail_plot5 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "HD Size (GB)", y = "Retail Price", title = "Retail Price compared to Hard Drive size", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(HD.Size..GB.)) + theme(text = element_text(size = 12))    

# Wireless

detail_plot6 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Integrated Wireless", y = "Retail Price", title = "Retail Price compared to Integrated Wireless", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Integrated.Wireless.)) + theme(text = element_text(size = 6))   

# Bundled Applications

detail_plot7 <- ggplot(Detail_Price) +
 aes(x = "", y = Retail.Price) +
 geom_violin(adjust = 1L, scale = "area", fill = "#1c6155") +
 labs(x = "Bundled Applications", y = "Retail Price", title = "Retail Price compared to Bundled Applications", 
 subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Bundled.Applications.)) + theme(text = element_text(size = 6))   

# All Together

grid.arrange(detail_plot1,detail_plot2,detail_plot3,detail_plot4,detail_plot6,detail_plot7)
grid.arrange(detail_plot5)

```

</center>

### ii. Do all stores sell all configurations?


```{r 15, include=FALSE}

Stores_Details <- Laptop_Sales_Data[,.(Store.Postcode,Configuration)]

Stores_Details <- na.omit(Stores_Details)

Stores_Details$Configuration <- as.integer(Stores_Details$Configuration)

```


<center>

```{r 151,echo=FALSE}

ggplotly(
ggplot(Stores_Details) +
 aes(x = Configuration) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 labs(x = "Stores ", 
 y = "Configurations Count", title = "Each Configurations per Stores", subtitle = "In 2018") +
 theme_classic() +
 facet_wrap(vars(Store.Postcode), scales = "free")
)

```

</center>



> With this multiple facets barplots, you can spot which configuration is less or not sold depending on the store.


# Ex 4.1

```{r a1, include=FALSE}

rm(list = ls()) # clean environment

```

## Loading Datas and Cleaning 

> Loading the dataset called "Cereals.csv" given for the Homework

```{r a2,echo=FALSE, warning=FALSE, comment=FALSE}

Cereals_Data <- fread("DATA/Cereals.csv")

#is.data.table(Cereals_Data)

str(Cereals_Data)

```


<center>


```{r a3, echo=FALSE, warning=FALSE, comment=FALSE}

gg_miss_var(Cereals_Data, show_pct = TRUE)

```

</center>

## a.

> Ordinal: shelf,vitamins

> Nominal: name, mfr, type    

> Quantitative/Numerical: calories,protein,fat,sodium ,sugars,potass,weight,cups,rating,fiber,carbo      


## b.

<center>

> Summary

```{r b1, echo=FALSE, warning=FALSE, comment=FALSE}

Cereals_Data <- na.omit(Cereals_Data)

b_result <- summary(Cereals_Data)

b_result

```



</center>

<center>

> Standard Errors 

```{r b11, echo=FALSE, warning=FALSE, comment=FALSE}

b_sd <- sapply(Cereals_Data,sd,na.rm=TRUE)

b_sd

```

</center>


## c.

<center>

> Histogram of Quantitative Variables

```{r b2, echo=FALSE, warning=FALSE, comment=FALSE}

Histo_Cereals_Data <- Cereals_Data[,c("calories","protein","fat","sodium" ,"sugars","potass","weight","cups","rating","fiber","carbo")]

Histo_Graph_1 <- ggplot(Histo_Cereals_Data) +
 aes(x = calories) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_2 <- ggplot(Histo_Cereals_Data) +
 aes(x = protein) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_3 <- ggplot(Histo_Cereals_Data) +
 aes(x = sugars) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_4 <- ggplot(Histo_Cereals_Data) +
 aes(x = potass) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_5 <- ggplot(Histo_Cereals_Data) +
 aes(x = weight) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_6 <- ggplot(Histo_Cereals_Data) +
 aes(x = cups) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_7 <- ggplot(Histo_Cereals_Data) +
 aes(x = rating) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_8 <- ggplot(Histo_Cereals_Data) +
 aes(x = fiber) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_9 <- ggplot(Histo_Cereals_Data) +
 aes(x = carbo) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_10 <- ggplot(Histo_Cereals_Data) +
 aes(x = sodium) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

Histo_Graph_11 <- ggplot(Histo_Cereals_Data) +
 aes(x = fat) +
 geom_histogram(bins = 30L, fill = "#1c6155") +
 theme_classic()

```

</center>

<center>


```{r b3, echo=FALSE, warning=FALSE, comment=FALSE}

grid.arrange(Histo_Graph_1,Histo_Graph_2,Histo_Graph_3,Histo_Graph_4,Histo_Graph_5,Histo_Graph_6,Histo_Graph_7,Histo_Graph_8,Histo_Graph_9,Histo_Graph_10,Histo_Graph_11)

```

</center>

<center>

> Standards Errors

```{r b4, echo=FALSE, warning=FALSE, comment=FALSE}

quantitative_sd <- sapply(Histo_Cereals_Data,sd,na.rm=TRUE)

quantitative_sd

```

</center>

### i. Which vairables have the largest variability?

> Based on the Histogram Grid and the Standard Errors Summary, Sodium, Potass and Calories have the largest variability.

### ii. Which variables seem skedew?

> Potassium, Rating, Fiber and Fat seem skewed. Sugar could also be. 

### iii. Are there any values that seem extreme? 

> We can see that Fiber has at least 2 extremes values (2 classes away from the main cluster), calories and weight as well. Cups could also have extremes values. 

## d.

<center>

```{r d, echo=FALSE, warning=FALSE, comment=FALSE}

ggplotly(
ggplot(Cereals_Data) +
 aes(x = type, y = calories) +
 geom_boxplot(fill = "#1c6155") +
 labs(x = "Type", 
 y = "Calories", title = "Calories in Hot VS Cold Cereals", subtitle = "Side-by-side Boxplots") +
 theme_classic()
)

```

</center>

> We are lacking data about Hot Type Cereals to compare it. 

## e.

<center>

```{r e, echo=FALSE, warning=FALSE, comment=FALSE}

ggplotly(
ggplot(Cereals_Data) +
 aes(x = "", y = rating) +
 geom_boxplot(fill = "#1c6155") +
 labs(x = "Shelf Height", 
 y = "Rating", title = "Rating grouped by Shelf Height", subtitle = "Side-by-side Boxplots") +
 theme_classic() +
 facet_wrap(vars(shelf))
)

```

</center>

> Shelf 1 and 3 are pretty close (both median close to 40-42), we could only take a mean of category 1 et 3 and then predict with category 2. 

## f.

### i. Which pair of variables is most strongly correlated?

<center>

> Correlation Matrix

```{r f, echo=FALSE, warning=FALSE, comment=FALSE}

Corr_Data <- Cereals_Data[,c("calories","protein","fat","sodium" ,"sugars","potass","weight","cups","rating","fiber","carbo")]

Corr_Data <- na.omit(Corr_Data)

Corr_plot <- cor(Corr_Data)

Corr_plot

```


</center>


<center>


```{r f1, echo=FALSE, warning=FALSE, comment=FALSE}

corrplot(Corr_plot, method = "color", col=brewer.pal(n=8, name="BuGn"),tl.col="black",tl.srt=45,addCoef.col = "black",number.cex = 0.5)

```

</center>

> Fiber and Potass seems to have a strong correlation

### ii. How can we reduce the number of variables based on these correlations? 

<center>

```{r f2, echo=FALSE, warning=FALSE, comment=FALSE}

cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}

p.mat <- cor.mtest(Corr_plot)

corrplot(Corr_plot, method = "color", col=brewer.pal(n=8, name="BuGn"),tl.col="black",tl.srt=45,addCoef.col = "black",number.cex = 0.5,  p.mat = p.mat, sig.level = 0.01, insig = "blank")

```

</center>

> By computing each p-value, we could remove unsignificant variables (if we are interesting in keeping the correlated variables amongst the dataset) with alpha=0.01 

### iii. How would the correlations change if we normalized the data first?

<center>

> Correlation Matrix

```{r f5, echo=FALSE, warning=FALSE, comment=FALSE}

Scale_data <- as.data.frame(scale(Corr_Data))

Corr_plot_scale <- cor(Scale_data)

Corr_plot_scale

```

</center>

<center>


```{r f6, echo=FALSE, warning=FALSE, comment=FALSE}

corrplot(Corr_plot_scale, method = "color", col=brewer.pal(n=8, name="PuBuGn"),tl.col="black",tl.srt=45,addCoef.col = "black",number.cex = 0.5)

```

</center>


<center>

```{r f7, echo=FALSE, warning=FALSE, comment=FALSE}

p.mat <- cor.mtest(Corr_plot_scale)

corrplot(Corr_plot_scale, method = "color", col=brewer.pal(n=8, name="PuBuGn"),tl.col="black",tl.srt=45,addCoef.col = "black",number.cex = 0.5,  p.mat = p.mat, sig.level = 0.01, insig = "blank")

```


</center>

> Nothing changes when we normalized the data before correlation matrices and plots








